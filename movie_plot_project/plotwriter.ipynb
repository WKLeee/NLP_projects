{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQVgowrfLUj3"
      },
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import CuDNNLSTM\n",
        "from keras.layers import Embedding\n",
        "from numpy import array\n",
        "import time\n",
        "from ibm_watson import DiscoveryV1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lgyv1yxMCnl"
      },
      "source": [
        "def get_discovery_data(director, origin, year, genre):\n",
        "    movies = []\n",
        "    # Enter Discovery ID\n",
        "    env_id = '' \n",
        "    col_id = ''\n",
        "    discovery = DiscoveryV1(\n",
        "        version = '2018-08-01',\n",
        "        # Enter Discovery Key\n",
        "        iam_apikey = ' '\n",
        "        url = 'https://gateway.watsonplatform.net/discovery/api'\n",
        "    )\n",
        "    num = 100\n",
        "    sentence = director\n",
        "    filterParam = None\n",
        "    response1 = discovery.query(environment_id = env_id, collection_id = col_id, filter = filterParam, query = sentence, count = num)\n",
        "    res = response1.result['results']\n",
        "    for movie in res:\n",
        "        if movie['result_metadata']['score']>6:\n",
        "            movies.append({'title':movie['Title'],'plot':movie['Plot']})\n",
        "    filterParam = 'Origin:'+origin\n",
        "    sentence = genre\n",
        "    response1 = discovery.query(environment_id = env_id, collection_id = col_id, filter = filterParam, query = sentence, count = num)\n",
        "    res = response1.result['results']\n",
        "    for movie in res:\n",
        "        if movie['Title'] in str(movies):\n",
        "            pass\n",
        "        else:\n",
        "            movies.append({'title':movie['Title'],'plot':movie['Plot']})\n",
        "            \n",
        "    return movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyQ86UCRMFc6"
      },
      "source": [
        "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        # encode the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pre-pad sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "        # predict probabilities for each word\n",
        "        yhat = model.predict_classes(encoded, verbose=0)\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += ' ' + out_word\n",
        "    return in_text\n",
        "\n",
        "def sequence_line(line, n):\n",
        "    sequences = list()\n",
        "    for i in range(n, len(line)) :\n",
        "        sequence = line[i-n: i]\n",
        "        sequences.append(sequence)\n",
        "    return sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnQn3_qqMHLi"
      },
      "source": [
        "def generate_model (title, year, genre, director, origin):\n",
        "    ngram = 3\n",
        "    epochs_num = 1000\n",
        "    limit = 100\n",
        "    gen = genre\n",
        "    genre = genre.replace(' ','')\n",
        "    genres = genre.split(',')\n",
        "\n",
        "    filename = genre + \"_\" + str(ngram) + \"_\" + str(epochs_num) + \"_model.h5\"\n",
        "    data = []\n",
        "    f = open('movies.json','r')\n",
        "    while True:\n",
        "        s = f.readline()\n",
        "        if len(s) < 1:\n",
        "            break\n",
        "        data.append(json.loads(s))\n",
        "    f.close()\n",
        "    print(\"There are \"+str(len(data))+\" movies\")\n",
        "    \n",
        "    year_data = []\n",
        "    for a in data:\n",
        "        if a[\"Year\"]>=str(int(year)-5) and a[\"Year\"]<str(int(year)+5):\n",
        "            year_data.append(a)\n",
        "        \n",
        "    random.shuffle(year_data)\n",
        "    print(\"There are \"+year+\"+-5 \"+str(len(year_data))+\" movies\")\n",
        "\n",
        "    limited_data = []\n",
        "    for pre_data in data:\n",
        "        for g in genres:\n",
        "            if g in pre_data[\"Genre\"]:\n",
        "                limited_data.append(pre_data)\n",
        "\n",
        "    limited_data = limited_data[0:limit]\n",
        "    \n",
        "    disdata = get_discovery_data(director, origin, year, gen)\n",
        "    plots = [movie['Plot'] for movie in limited_data]\n",
        "    for d in disdata:\n",
        "        plots.append(d['plot'])\n",
        "    \n",
        "    print(\"Using {0} movie plots\".format(len(plots)))\n",
        "        \n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(plots)\n",
        "    encoded = tokenizer.texts_to_sequences(plots)\n",
        "    \n",
        "    vocab_size = len(tokenizer.word_index)+1\n",
        "    print('vocab size = {0}'.format(vocab_size))\n",
        "    \n",
        "    print('processing {0}_grams'.format(ngram))\n",
        "    \n",
        "    sequences = list()\n",
        "    for line in encoded:\n",
        "        sequences.extend(sequence_line(line, ngram))\n",
        "\n",
        "    sequences = array(sequences)\n",
        "    x, y = sequences[:,0:-1], sequences[:,-1]\n",
        "    \n",
        "    model = None\n",
        "    title = title.replace(' ','')\n",
        "    filename = title + \"_\" + str(ngram) + \"_\" + str(epochs_num) + \"_model.h5\"\n",
        "    if(os.path.isfile(filename)):\n",
        "       model = load_model(filename)\n",
        "    if model is None:\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length = ngram-1))\n",
        "        model.add(LSTM(50))\n",
        "        model.add(Dense(vocab_size, activation='softmax'))\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        model.fit(x,y, epochs=epochs_num, verbose=2, batch_size=256)\n",
        "        model.save(filename)\n",
        "        \n",
        "    return model, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwxq7s_qMJo0"
      },
      "source": [
        "# Implementation\n",
        "year = '1997'\n",
        "genre = 'Drama, Romance'\n",
        "ngram = 3\n",
        "genre = genre.lower()\n",
        "title = 'Good Will Hunting'\n",
        "director = 'Gus Van Sant'\n",
        "origin = 'American'\n",
        "\n",
        "model, tokenizer = generate_model (title, year, genre, director, origin)\n",
        "print(generate_seq(model, tokenizer, ngram-1, 'A rich man', 24))\n",
        "print(generate_seq(model, tokenizer, ngram-1, 'He loves', 24))\n",
        "print(generate_seq(model, tokenizer, ngram-1, 'They married in', 10))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}